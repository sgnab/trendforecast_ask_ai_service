# src/interpret_query_v2.py

import logging
import os
import json
import csv
from pathlib import Path
from typing import Dict, List, Any, Optional, Set

# AWS SDK for Secrets Manager
import boto3
from botocore.exceptions import ClientError # Import specific exception

# LLM SDK
try:
    import google.generativeai as genai
    GEMINI_SDK_AVAILABLE = True
except ImportError:
    genai = None
    GEMINI_SDK_AVAILABLE = False
    # Log error early if SDK is missing
    logging.basicConfig(level="ERROR") # Ensure logging is configured even if handler fails early
    logging.error("CRITICAL: google-generativeai SDK not found! Install it.")

# --- Configuration ---
# Assume CSV files are in the 'config_data' subdirectory relative to this file
LAMBDA_ROOT = Path(__file__).resolve().parent
CONFIG_DIR = LAMBDA_ROOT / "config_data"
CATEGORIES_CSV = CONFIG_DIR / "categories.csv"
STYLES_CSV = CONFIG_DIR / "styles.csv"       # Expects single column, header 'styles'
COLORS_CSV = CONFIG_DIR / "colors.csv"       # Expects single column, header 'colors'

# Get config from environment variables
SECRET_NAME = os.environ.get("SECRET_NAME", "YourGeminiSecretName") # e.g., GeminiAPIKeySecret
LLM_MODEL_NAME = os.environ.get("INTERPRET_LLM_MODEL", "gemini-1.5-flash-latest")
AWS_REGION = os.environ.get("AWS_REGION", "us-west-2") # Sensible default

# --- Initialize Logger ---
logger = logging.getLogger()
log_level_str = os.environ.get("LOG_LEVEL", "INFO").upper()
# Validate log level string
valid_log_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
if log_level_str not in valid_log_levels:
    log_level_str = "INFO" # Default to INFO if invalid level provided
logger.setLevel(log_level_str)
logger.info(f"Logger initialized with level: {log_level_str}")


# --- Globals for Loaded Config Data ---
KNOWN_CATEGORIES: Set[str] = set()
KNOWN_STYLES: Set[str] = set() # Single set for all styles
KNOWN_COLORS: Set[str] = set() # Single set for all colors
CONFIG_LOAD_ERROR: Optional[str] = None # Store potential loading error message

# --- Revised Function to Load Config Data ---
def load_config_csvs():
    """Loads known lists from single-column CSVs. Populates globals."""
    global KNOWN_CATEGORIES, KNOWN_STYLES, KNOWN_COLORS, CONFIG_LOAD_ERROR
    logger.info(f"Attempting to load config data from: {CONFIG_DIR}")

    # Clear existing data
    KNOWN_CATEGORIES.clear()
    KNOWN_STYLES.clear()
    KNOWN_COLORS.clear()
    CONFIG_LOAD_ERROR = None

    try:
        # --- Load Categories ---
        if not CATEGORIES_CSV.is_file():
            msg = f"CRITICAL: Categories CSV not found at {CATEGORIES_CSV}"
            logger.error(msg); CONFIG_LOAD_ERROR = msg; return
        with open(CATEGORIES_CSV, mode='r', encoding='utf-8-sig') as infile:
            reader = csv.reader(infile)
            try:
                header = next(reader) # Skip header (assume 'category')
                logger.debug(f"Categories CSV header: {header}")
                count = 0
                for row in reader:
                    if row and row[0].strip():
                        KNOWN_CATEGORIES.add(row[0].strip().lower())
                        count += 1
                logger.info(f"Loaded {count} categories.")
                if count == 0: logger.warning(f"'{CATEGORIES_CSV.name}' contained no data rows.")
            except StopIteration: logger.warning(f"'{CATEGORIES_CSV.name}' empty or only header.")
            except IndexError: logger.error(f"Format error in {CATEGORIES_CSV.name}."); CONFIG_LOAD_ERROR = f"Format error in {CATEGORIES_CSV.name}"; return

        # --- Load Styles ---
        if STYLES_CSV.is_file():
            with open(STYLES_CSV, mode='r', encoding='utf-8-sig') as infile:
                reader = csv.reader(infile)
                try:
                    header = next(reader) # Skip header (assume 'styles')
                    logger.debug(f"Styles CSV header: {header}")
                    count = 0
                    for row in reader:
                        if row and row[0].strip():
                            style = row[0].strip().lower()
                            if style not in KNOWN_STYLES: # Add only unique styles
                                KNOWN_STYLES.add(style)
                                count += 1
                    logger.info(f"Loaded {count} unique styles.")
                    if count == 0: logger.warning(f"'{STYLES_CSV.name}' contained no data rows.")
                except StopIteration: logger.warning(f"'{STYLES_CSV.name}' empty or only header.")
                except IndexError: logger.error(f"Format error in {STYLES_CSV.name}."); CONFIG_LOAD_ERROR = f"Format error in {STYLES_CSV.name}"; return
        else:
            logger.warning(f"Styles CSV not found at {STYLES_CSV}, style checking unavailable.")

        # --- Load Colors ---
        if COLORS_CSV.is_file():
             with open(COLORS_CSV, mode='r', encoding='utf-8-sig') as infile:
                reader = csv.reader(infile)
                try:
                    header = next(reader) # Skip header (assume 'colors')
                    logger.debug(f"Colors CSV header: {header}")
                    count = 0
                    for row in reader:
                         if row and row[0].strip():
                             color = row[0].strip().lower()
                             if color not in KNOWN_COLORS: # Add only unique colors
                                 KNOWN_COLORS.add(color)
                                 count += 1
                    logger.info(f"Loaded {count} unique colors.")
                    if count == 0: logger.warning(f"'{COLORS_CSV.name}' contained no data rows.")
                except StopIteration: logger.warning(f"'{COLORS_CSV.name}' empty or only header.")
                except IndexError: logger.error(f"Format error in {COLORS_CSV.name}."); CONFIG_LOAD_ERROR = f"Format error in {COLORS_CSV.name}"; return
        else:
             logger.warning(f"Colors CSV not found at {COLORS_CSV}, color checking unavailable.")

    except FileNotFoundError as e: logger.error(f"Config loading failed: {e}"); CONFIG_LOAD_ERROR = str(e)
    except Exception as e: logger.exception("CRITICAL ERROR loading config CSVs!"); CONFIG_LOAD_ERROR = f"Unexpected error loading config CSVs: {e}"

    logger.debug(f"Final loaded category count: {len(KNOWN_CATEGORIES)}")
    logger.debug(f"Final loaded style count: {len(KNOWN_STYLES)}")
    logger.debug(f"Final loaded color count: {len(KNOWN_COLORS)}")

# --- Load config data when module is first imported by Lambda runtime ---
load_config_csvs()

# --- Initialize Boto3 Client (Globally) ---
secrets_manager = None
BOTO3_CLIENT_ERROR = None
try:
    session = boto3.session.Session()
    secrets_manager = session.client(
        service_name='secretsmanager',
        region_name=AWS_REGION
    )
except Exception as e:
    logger.exception("CRITICAL ERROR initializing Boto3 client!")
    BOTO3_CLIENT_ERROR = f"Failed to initialize Boto3 client: {e}"


# --- API Key Caching (Globally) ---
API_KEY_CACHE: Dict[str, Optional[str]] = {} # Initialize empty

# --- Helper Function to Get and Cache Secrets ---
def get_secret_value(secret_name: str, key_name: str) -> Optional[str]:
    """
    Retrieves a specific key from a secret in AWS Secrets Manager.
    Expects the secret value to be a JSON string. Caches the result.
    Returns the secret key's value string or None on error.
    """
    is_local = os.environ.get("IS_LOCAL", "false").lower() == "true"
    if is_local:
        direct_key = os.environ.get(key_name)
        if direct_key:
            logger.info(f"Using direct environment variable for key: {key_name} (local mode)")
            return direct_key
        else:
            logger.warning(f"Direct env var {key_name} not found in local mode. Falling back to Secrets Manager.")
    global API_KEY_CACHE

    if key_name in API_KEY_CACHE and API_KEY_CACHE[key_name]:
        logger.debug(f"Using cached secret key: {key_name}")
        return API_KEY_CACHE[key_name]

    if BOTO3_CLIENT_ERROR:
         logger.error(f"Cannot fetch secrets, Boto3 client error: {BOTO3_CLIENT_ERROR}")
         return None
    if not secrets_manager:
         logger.error("Cannot fetch secrets, Secrets Manager client not initialized.")
         return None

    try:
        logger.info(f"Fetching secret '{secret_name}' to get key '{key_name}'")
        get_secret_value_response = secrets_manager.get_secret_value(
            SecretId=secret_name
        )

        secret_dict = None
        if 'SecretString' in get_secret_value_response:
            secret_string = get_secret_value_response['SecretString']
            try:
                secret_dict = json.loads(secret_string)
            except json.JSONDecodeError as json_err:
                logger.error(f"Failed to parse SecretString JSON for {secret_name}: {json_err}")
                return None
        elif 'SecretBinary' in get_secret_value_response:
             logger.warning("Secret is binary, attempting to decode as JSON.")
             secret_binary = get_secret_value_response['SecretBinary']
             try:
                 secret_dict = json.loads(secret_binary.decode('utf-8'))
             except (json.JSONDecodeError, UnicodeDecodeError) as decode_err:
                 logger.error(f"Failed to decode binary secret: {decode_err}")
                 return None
        else:
            logger.error(f"Secret value not found in response for {secret_name}")
            return None

        if not isinstance(secret_dict, dict):
             logger.error(f"Parsed secret for {secret_name} is not a dictionary.")
             return None

        # Extract the specific key
        key_value = secret_dict.get(key_name)

        if not key_value or not isinstance(key_value, str): # Ensure key exists and is a string
            logger.error(f"Key '{key_name}' not found or not a string within secret JSON in '{secret_name}'")
            # Optionally cache the fact that it's missing/invalid? For now, just return None.
            return None

        # Cache the specific key retrieved
        API_KEY_CACHE[key_name] = key_value
        logger.info(f"Key '{key_name}' successfully retrieved from secret and cached.")
        return key_value

    except ClientError as e:
        # Handle specific AWS errors
        error_code = e.response.get("Error", {}).get("Code")
        error_message = e.response.get("Error", {}).get("Message", str(e))
        logger.error(f"AWS ClientError retrieving secret '{secret_name}': {error_code} - {error_message}")
        if error_code == 'ResourceNotFoundException':
            logger.error(f"Secret '{secret_name}' not found.")
        elif error_code == 'AccessDeniedException':
            logger.error(f"Access denied retrieving secret '{secret_name}'. Check Lambda IAM permissions.")
        # Add more specific error handling if needed
        return None
    except Exception as e:
        logger.exception(f"Unexpected error retrieving or parsing secret '{secret_name}'.")
        return None


# --- Main Lambda Handler ---
# --- [ CODE BEFORE lambda_handler ] ---
# ... (imports, config loading, helpers - remain the same as your provided code) ...

# --- Main Lambda Handler ---
def lambda_handler(event, context):
    # ... (initial checks, input parsing, API key retrieval, LLM setup - remain the same) ...

        category_lower = category.lower()
        # Use the globally loaded sets, ensure Title Case for display in prompt
        known_styles_list = sorted([s.title() for s in KNOWN_STYLES])
        known_colors_list = sorted([c.title() for c in KNOWN_COLORS])

        # Construct the Prompt (MODIFIED Instruction #3 and Output Structure)
        prompt = f"""Analyze the user query strictly within the given fashion context.
        Context:
        - Category: "{category.title()}"
        - Country: "{country.title()}"
        - List of All Known Styles (global): {json.dumps(known_styles_list) if known_styles_list else "None Provided"}
        - List of All Known Colors (global): {json.dumps(known_colors_list) if known_colors_list else "None Provided"}

        User Query: "{user_query}"

        Instructions:
        1.  Identify the primary analysis task based on the User Query's intent (e.g., forecast, trends, recommendations, comparison). Choose ONE from: ['get_trend', 'get_forecast', 'get_recommendation', 'compare_items', 'summarize_category', 'summarize_mega_trends', 'qa_web_only', 'qa_internal_only', 'qa_combined', 'unknown'].
        2.  Determine the *minimum necessary* data sources required for the task. Choose one or more from: ['internal_trends_category', 'internal_trends_item', 'internal_forecast', 'internal_mega', 'web_search', 'clarify'].
            - If specific subjects (items/styles/colors) are identified AND the task requires item-level detail (like 'get_forecast', 'get_recommendation' for an item, 'compare_items'), include 'internal_trends_item' and/or 'internal_forecast'.
            - If the task is broad (like 'summarize_category'), use 'internal_trends_category'.
            - Use 'internal_mega' if keywords suggest rising/hot trends.
            - Include 'web_search' if the query explicitly mentions news, sentiment, competitors, asks 'why', or seems unanswerable by internal data.
            - If the query is too ambiguous, invalid, lacks specifics needed for the task, or falls outside the Category/Country context, use ONLY 'clarify'.
        3.  Extract key entities mentioned in the User Query:
            -   `specific_known_subjects`: Create a list of objects. For each item/style/color from the query that *exactly matches* (case-insensitive) ANY entry in the 'All Known Styles' or 'All Known Colors' lists:
                a. Determine if the matched subject is appropriate for the stated Category context (e.g., "maxi dress" is likely inappropriate for Category "Shirts").
                b. **If appropriate:** Determine its type ('color' or 'style').
                c. **If appropriate:** Add an object to the list with keys "subject" (the matched term, formatted in Title Case) and "type" (either "color" or "style"). Example: `{{"subject": "Blue", "type": "color"}}`.
                d. **If inappropriate for the category:** Do NOT add it to this list; add the term to `unmapped_items` instead.
            -   `unmapped_items`: List any items/keywords from the query that look like fashion terms but were NOT found in the known lists OR were found but deemed inappropriate for the stated Category context. Use Title Case for items in this list.
            -   `timeframe_reference`: Any mention of time (e.g., "next 3 months", "last year", "latest"). Return null if none found.
            -   `attributes`: List any other descriptive attributes mentioned (e.g., "material:linen", "price:high", "fit:baggy"). Return [] if none found.
        4.  Determine the overall 'status'. It MUST be 'needs_clarification' if 'clarify' is in required_sources OR if step 3 identified potentially relevant but category-inappropriate subjects. Otherwise, it MUST be 'success'.
        5.  Provide a concise 'clarification_needed' message (string) ONLY if status is 'needs_clarification', otherwise it MUST be null. Include reasons like ambiguity or category mismatch if applicable.

        Output ONLY a valid JSON object following this exact structure:
        {{
          "status": "success | needs_clarification",
          "primary_task": "string | null",
          "required_sources": ["string", ...],
          "query_subjects": {{
            "specific_known": [ {{ "subject": "string (Title Case)", "type": "color | style" }} ],
            "unmapped_items": ["string (Title Case)", ...]
          }},
          "timeframe_reference": "string | null",
          "attributes": ["string", ...],
          "clarification_needed": "string | null"
        }}
        """
        logger.debug("Prompt constructed.")

        # 4. Call LLM
        # ... (LLM call logic remains the same) ...
        logger.info(f"Calling LLM: {LLM_MODEL_NAME}...")
        try:
            generation_config = genai.types.GenerationConfig(response_mime_type="application/json")
            response = model.generate_content(prompt, generation_config=generation_config)
            logger.info("LLM response received.")
            logger.debug(f"LLM Raw Response Text:\n{response.text}")
        except Exception as llm_err:
             logger.error(f"LLM API call failed: {llm_err}", exc_info=True)
             return {"statusCode": 502, "body": json.dumps({"status": "error", "error_message": f"LLM API call failed: {llm_err}"})}

        # 5. Parse and Validate LLM Response (MODIFIED Validation for specific_known)
        try:
            cleaned_text = response.text.strip()
            if cleaned_text.startswith("```json"): cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith("```"): cleaned_text = cleaned_text[:-3]
            cleaned_text = cleaned_text.strip()
            if not cleaned_text: raise ValueError("LLM returned empty response after cleaning markdown.")

            llm_output = json.loads(cleaned_text)

            # --- Validation ---
            required_keys = ["status", "primary_task", "required_sources", "query_subjects", "timeframe_reference", "attributes", "clarification_needed"]
            missing_keys = [key for key in required_keys if key not in llm_output]
            if missing_keys: raise ValueError(f"LLM output missing required keys: {', '.join(missing_keys)}.")

            if not isinstance(llm_output.get("required_sources"), list): raise ValueError("LLM output 'required_sources' is not a list.")

            query_subjects = llm_output.get("query_subjects")
            if not isinstance(query_subjects, dict): raise ValueError("LLM output 'query_subjects' is not a dictionary.")
            if "specific_known" not in query_subjects or "unmapped_items" not in query_subjects: raise ValueError("LLM output 'query_subjects' missing 'specific_known' or 'unmapped_items'.")
            if not isinstance(query_subjects["unmapped_items"], list): raise ValueError("LLM output 'query_subjects.unmapped_items' is not a list.")

            # *** MODIFIED VALIDATION for specific_known ***
            specific_known = query_subjects["specific_known"]
            if not isinstance(specific_known, list):
                raise ValueError("LLM output 'query_subjects.specific_known' is not a list.")
            # Check if all elements in the list are dictionaries with required keys
            for item in specific_known:
                if not isinstance(item, dict):
                    raise ValueError(f"Item in 'specific_known' is not a dictionary: {item}")
                if "subject" not in item or "type" not in item:
                    raise ValueError(f"Item in 'specific_known' missing 'subject' or 'type' key: {item}")
                if item["type"] not in ["color", "style"]:
                     raise ValueError(f"Item in 'specific_known' has invalid type '{item['type']}': {item}")
                # Optional: Check if subject is string?
                if not isinstance(item.get("subject"), str):
                     raise ValueError(f"Subject in 'specific_known' item is not a string: {item}")
            # *** END MODIFIED VALIDATION ***

            # Check status consistency
            if "clarify" in llm_output.get("required_sources", []) and llm_output.get("status") != "needs_clarification":
                logger.warning("LLM required 'clarify' source but status is not 'needs_clarification'. Forcing status.")
                llm_output["status"] = "needs_clarification"
            if llm_output.get("status") == "needs_clarification" and not llm_output.get("clarification_needed"):
                 logger.warning("LLM status is 'needs_clarification' but no message provided. Adding generic message.")
                 llm_output["clarification_needed"] = "Query requires clarification. Please be more specific or ensure terms are relevant to the category." # Updated generic message

            logger.info(f"LLM interpretation successful. Task: {llm_output.get('primary_task')}, Status: {llm_output.get('status')}")

            # Add original context back for downstream use (ensure Title Case consistency if desired)
            llm_output['original_context'] = {
                'category': category.title(), # Ensure title case
                'country': country.title(),   # Ensure title case
                'query': user_query
             }

            # 6. Return Success Response
            return { "statusCode": 200, "body": json.dumps(llm_output) }

        except (json.JSONDecodeError, ValueError, TypeError) as e:
             # ... (Error handling for parsing/validation failure remains the same) ...
             logger.error(f"Failed to parse or validate LLM JSON response: {e}", exc_info=True)
             logger.error(f"LLM Raw Text (at parse failure): {response.text}")
             return { "statusCode": 500, "body": json.dumps({"status": "error", "error_message": f"Failed processing LLM response: {e}", "llm_raw_output": response.text}) }

    except Exception as e:
        # ... (Catch-all error handling remains the same) ...
        logger.exception("Unhandled error processing interpretation request.")
        return { "statusCode": 500, "body": json.dumps({"status": "error", "error_message": f"Internal server error: {str(e)}"}) }